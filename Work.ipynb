{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Description\n",
    "\n",
    "ph: pH of 1. water (0 to 14).\n",
    "\n",
    "Hardness: Capacity of water to precipitate soap in mg/L.\n",
    "\n",
    "Solids: Total dissolved solids in ppm.\n",
    "\n",
    "Chloramines: Amount of Chloramines in ppm.\n",
    "\n",
    "Sulfate: Amount of Sulfates dissolved in mg/L.\n",
    "\n",
    "Conductivity: Electrical conductivity of water in μS/cm.\n",
    "\n",
    "Organic_carbon: Amount of organic carbon in ppm.\n",
    "\n",
    "Trihalomethanes: Amount of Trihalomethanes in μg/L.\n",
    "\n",
    "Turbidity: Measure of light emiting property of water in NTU.\n",
    "\n",
    "Potability: Indicates if water is safe for human consumption. Potable - 1 and Not potable - 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),  \u001b[38;5;66;03m# Scaling the features\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m'\u001b[39m, SVC(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))  \u001b[38;5;66;03m# Support Vector Classifier\u001b[39;00m\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 2- RandomizedSearchCV for broad hyperparameter exploration\u001b[39;00m\n\u001b[0;32m      8\u001b[0m param_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc__kernel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Try both linear and RBF kernels\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc__C\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mstats\u001b[49m\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m100\u001b[39m),  \u001b[38;5;66;03m# Uniform distribution for C (regularization parameter)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc__gamma\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m]  \u001b[38;5;66;03m# Gamma for RBF kernel\u001b[39;00m\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Perform RandomizedSearchCV to explore the hyperparameter space\u001b[39;00m\n\u001b[0;32m     15\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(pipeline, param_dist, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stats' is not defined"
     ]
    }
   ],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "from collections import Counter\n",
    "\n",
    "# Visualizations Libraries\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "# Data Pre-processing Libraries\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modelling Libraries\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier,SGDClassifier,PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier,NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Evaluation & CV Libraries\n",
    "from sklearn.metrics import precision_score,accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dark = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\n",
    "colors_blue = [\"#132C33\", \"#264D58\", '#17869E', '#51C4D3', '#B4DBE9']\n",
    "colors_pink = ['#8B0046', '#C71585', '#E75480', '#FF91AF', '#FFB6C1']\n",
    "\n",
    "\n",
    "sns.palplot(colors_dark)\n",
    "sns.palplot(colors_blue)\n",
    "sns.palplot(colors_pink)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./water_potability.csv')\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "\n",
    "# profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "# profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = df['Potability'].value_counts()\n",
    "\n",
    "fig = px.pie(d, values='count', names=['Not Potable', 'Potable'], hole=0.4, opacity=0.6,\n",
    "             color_discrete_sequence=[colors_pink[3], colors_blue[3]],\n",
    "             labels={'label': 'Potability', 'count': 'No. Of Samples'})\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Q. How many samples of water are Potable?',x=0.47,y=0.98,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    legend=dict(x=0.37,y=-0.05,orientation='h',traceorder='reversed'),\n",
    "    hoverlabel=dict(bgcolor='white'))\n",
    "\n",
    "fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for class imbalance in Potability\n",
    "potability_counts = df['Potability'].value_counts()\n",
    "print(potability_counts)\n",
    "\n",
    "# Plot the distribution of Potability\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=potability_counts.index, y=potability_counts.values, palette=[colors_pink[3], colors_blue[3]])\n",
    "plt.title(\"Potability Class Distribution\")\n",
    "plt.xlabel(\"Potability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Water Hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Hardness',y=Counter(df['Hardness']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[4],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.add_vline(x=151, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "fig.add_vline(x=301, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "fig.add_vline(x=76, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "\n",
    "fig.add_annotation(text='<76 mg/L is<br> considered soft',x=40,y=130,showarrow=False,font_size=9)\n",
    "fig.add_annotation(text='Between 76 and 150<br> (mg/L) is<br>moderately hard',x=113,y=130,showarrow=False,font_size=9)\n",
    "fig.add_annotation(text='Between 151 and 300 (mg/L)<br> is considered hard',x=250,y=130,showarrow=False,font_size=9)\n",
    "fig.add_annotation(text='>300 mg/L is<br> considered very hard',x=340,y=130,showarrow=False,font_size=9)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Hardness Distribution',x=0.53,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='Hardness (mg/L)',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Water PH level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='ph',y=Counter(df['ph']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[3],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.add_vline(x=7, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "\n",
    "fig.add_annotation(text='<7 is Acidic',x=4,y=70,showarrow=False,font_size=10)\n",
    "fig.add_annotation(text='>7 is Basic',x=10,y=70,showarrow=False,font_size=10)\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='pH Level Distribution',x=0.5,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='pH Level',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TDS: TDS means concentration of dissolved particles or solids in water. TDS comprises of inorganic salts such as calcium, magnesium, chlorides, sulfates, bicarbonates, etc, along with many more inorganic compounds that easily dissolve in water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Solids',y=Counter(df['Solids']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[3],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Distribution Of Total Dissolved Solids',x=0.5,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='Dissolved Solids (ppm)',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chloramines: Chloramines (also known as secondary disinfection) are disinfectants used to treat drinking water and they:\n",
    "\n",
    "Are most commonly formed when ammonia is added to chlorine to treat drinking water.\n",
    "Provide longer-lasting disinfection as the water moves through pipes to consumers.\n",
    "Chloramines have been used by water utilities since the 1930s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Chloramines',y=Counter(df['Chloramines']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[3],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.add_vline(x=4, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "\n",
    "fig.add_annotation(text='<4 ppm is considered<br> safe for drinking',x=1.8,y=90,showarrow=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Chloramines Distribution',x=0.53,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='Chloramines (ppm)',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sulfate: Sulfate (SO4) can be found in almost all natural water. The origin of most sulfate compounds is the oxidation of sulfite ores, the presence of shales, or the industrial wastes. Sulfate is one of the major dissolved components of rain. High concentrations of sulfate in the water we drink can have a laxative effect when combined with calcium and magnesium, the two most common constituents of hardness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Sulfate',y=Counter(df['Sulfate']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[3],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.add_vline(x=250, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "\n",
    "fig.add_annotation(text='<250 mg/L is considered<br> safe for drinking',x=175,y=90,showarrow=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Sulfate Distribution',x=0.53,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='Sulfate (mg/L)',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conductivity: Conductivity is a measure of the ability of water to pass an electrical current. Because dissolved salts and other inorganic chemicals conduct electrical current, conductivity increases as salinity increases. Organic compounds like oil do not conduct electrical current very well and therefore have a low conductivity when in water. Conductivity is also affected by temperature: the warmer the water, the higher the conductivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Conductivity',y=Counter(df['Conductivity']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[3],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.add_annotation(text='The Conductivity range <br> is safe for both (200-800),<br> Potable and Non-Potable water',\n",
    "                   x=600,y=90,showarrow=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Conductivity Distribution',x=0.5,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='Conductivity (μS/cm)',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organic Carbon: Organic contaminants (natural organic substances, insecticides, herbicides, and other agricultural chemicals) enter waterways in rainfall runoff. Domestic and industrial wastewaters also contribute organic contaminants in various amounts. As a result of accidental spills or leaks, industrial organic wastes may enter streams. Some of the contaminants may not be completely removed by treatment processes; therefore, they could become a problem for drinking water sources. It is important to know the organic content in a waterway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Organic_carbon',y=Counter(df['Organic_carbon']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[3],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.add_vline(x=10, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "\n",
    "fig.add_annotation(text='Typical Organic Carbon<br> level is upto 10 ppm',x=5.3,y=110,showarrow=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Organic Carbon Distribution',x=0.5,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='Organic Carbon (ppm)',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trihalomethanes: Trihalomethanes (THMs) are the result of a reaction between the chlorine used for disinfecting tap water and natural organic matter in the water. At elevated levels, THMs have been associated with negative health effects such as cancer and adverse reproductive outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Trihalomethanes',y=Counter(df['Trihalomethanes']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[3],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.add_vline(x=80, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "\n",
    "fig.add_annotation(text='Upper limit of Trihalomethanes<br> level is 80 μg/L',x=115,y=90,showarrow=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Trihalomethanes Distribution',x=0.5,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='Trihalomethanes (μg/L)',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turbidity: Turbidity is the measure of relative clarity of a liquid. It is an optical characteristic of water and is a measurement of the amount of light that is scattered by material in the water when a light is shined through the water sample. The higher the intensity of scattered light, the higher the turbidity. Material that causes water to be turbid include clay, silt, very tiny inorganic and organic matter, algae, dissolved colored organic compounds, and plankton and other microscopic organisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,x='Turbidity',y=Counter(df['Turbidity']),color='Potability',template='plotly_white',\n",
    "                  marginal='box',opacity=0.7,nbins=100,color_discrete_sequence=[colors_pink[3],colors_blue[3]],\n",
    "                  barmode='group',histfunc='count')\n",
    "\n",
    "fig.add_vline(x=5, line_width=1, line_color=colors_dark[1],line_dash='dot',opacity=0.7)\n",
    "\n",
    "fig.add_annotation(text='<5 NTU Turbidity is<br> considered safe',x=6,y=90,showarrow=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Turbidity Distribution',x=0.5,y=0.95,\n",
    "               font=dict(color=colors_dark[2],size=20)),\n",
    "    xaxis_title_text='Turbidity (NTU)',\n",
    "    yaxis_title_text='Count',\n",
    "    legend=dict(x=1,y=0.96,bordercolor=colors_dark[4],borderwidth=0,tracegroupgap=5),\n",
    "    bargap=0.3,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying the correlation between the different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(df,df.drop('Potability',axis=1),height=1250,width=1250,template='plotly_white',opacity=0.7,\n",
    "                        color_discrete_sequence=[colors_blue[2],colors_pink[4]],color='Potability',\n",
    "                       symbol='Potability',color_continuous_scale=[colors_pink[1],colors_blue[4]])\n",
    "\n",
    "fig.update_layout(font_size=10,\n",
    "                  coloraxis_showscale=False,\n",
    "                 legend=dict(x=0.02,y=1.07,bgcolor=colors_dark[4]),\n",
    "                 title=dict(text='Scatter Plot Matrix b/w Features',x=0.5,y=0.97,\n",
    "                   font=dict(color=colors_dark[2],size=24)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor=df.drop('Potability',axis=1).corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is little to no correlation between the attributes. Which leads us to the conclusion that no Multicolineaity issue is faced in our case. The predictors are not strongly correlated, so let’s keep them all as they are!\n",
    "\n",
    "\n",
    "****The absence of correlations generally indicates that multicollinearity is not an issue in your data.\n",
    "This simplifies model building, improves the interpretability of coefficients, and enhances the stability and reliability of your models.\n",
    "However, always validate this assumption with diagnostics like VIF, even if initial correlations suggest low multicollinearity.\n",
    "\n",
    "\n",
    "\n",
    "To visualize better this, find the next heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a custom colormap from the specified pink colors\n",
    "pink_cmap = LinearSegmentedColormap.from_list('pink_cmap', colors_pink)\n",
    "\n",
    "# Define the function to plot the correlation heatmap\n",
    "def correlation_heatmap(df):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap=pink_cmap, annot_kws={ \"fontsize\": 10})\n",
    "    plt.title('Correlation Heatmap',fontdict={ 'fontsize': 15})\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your DataFrame\n",
    "correlation_heatmap(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix to observe interactions between features\n",
    "sns.pairplot(df, hue=\"Potability\", diag_kind=\"kde\",  palette=[colors_pink[3], colors_blue[2]])\n",
    "plt.show()\n",
    "\n",
    "# Comparing feature distributions for Potability (0 vs 1)\n",
    "for column in df.columns[:-1]:  # excluding 'Potability'\n",
    "    plt.figure(figsize=(8, 4), )\n",
    "    sns.kdeplot(df[column][df['Potability'] == 0], label='Not Potable', fill=True,  color= colors_pink[3])\n",
    "    sns.kdeplot(df[column][df['Potability'] == 1], label='Potable', fill=True,  color=colors_blue[2])\n",
    "    plt.title(f'{column} Distribution for Potability Classes')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPPING\n",
    "\n",
    "1- Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "print(\"Missing Data:\")\n",
    "print(missing_data)\n",
    "print(\"\\nMissing Data Percentage:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "\n",
    "# Visualizing missing data\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=pink_cmap)\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate potable and non-potable samples\n",
    "potable_samples = df[df['Potability'] == 1]\n",
    "non_potable_samples = df[df['Potability'] == 0]\n",
    "\n",
    "# Calculate missing percentages for potable samples\n",
    "missing_potable = potable_samples.isnull().sum() / len(potable_samples) * 100\n",
    "missing_potable = missing_potable[missing_potable > 0]  # Filter out features with no missing data\n",
    "\n",
    "# Calculate missing percentages for non-potable samples\n",
    "missing_non_potable = non_potable_samples.isnull().sum() / len(non_potable_samples) * 100\n",
    "missing_non_potable = missing_non_potable[missing_non_potable > 0]  # Filter out features with no missing data\n",
    "\n",
    "# Create dataframes for plotting\n",
    "missing_potable_df = pd.DataFrame({\n",
    "    'variable': missing_potable.index,\n",
    "    'missing_rate': missing_potable.values,\n",
    "    'potability': 'Potable'\n",
    "})\n",
    "\n",
    "missing_non_potable_df = pd.DataFrame({\n",
    "    'variable': missing_non_potable.index,\n",
    "    'missing_rate': missing_non_potable.values,\n",
    "    'potability': 'Non-Potable'\n",
    "})\n",
    "\n",
    "# Combine the two dataframes for plotting\n",
    "missing_combined_df = pd.concat([missing_potable_df, missing_non_potable_df])\n",
    "\n",
    "# Plot the missing rate for potable and non-potable samples individually\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=missing_combined_df, x='missing_rate', y='variable', hue='potability', palette=['#D41C64', '#4B6FB5'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Missing Rate by Feature for Potable and Non-Potable Samples', fontsize=16)\n",
    "plt.xlabel('Missing Rate (%)')\n",
    "plt.ylabel('Features')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are missing data in the visualization above. Those need to be adressed. There are a few ways I can go about this: removing the missing observations or replace with the median/mean...\n",
    "I would rather not delete the missing data observations because that will reduce the dataset size significantly and this may harm the models we will adress later on. Therefore we will look to replace them.\n",
    "I'll start by measuring the medians of potable and non-potable observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Potability']==0].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Potability']==0][['ph','Sulfate','Trihalomethanes']].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Potability']==1].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Potability']==1][['ph','Sulfate','Trihalomethanes']].median()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the difference between the median values of Potable and Non-Potable Water is minimal. So we use the overall median of each attribute to replace the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({'Sulfate': df['Sulfate'].median()}, inplace=True)\n",
    "df.fillna({'Trihalomethanes': df['Trihalomethanes'].median()}, inplace=True)\n",
    "df.fillna({'ph': df['ph'].median()}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we have replaced all our null and we possess a complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Outliers\n",
    "\n",
    "I have two options:\n",
    "Remove them: I am not sure about the benefit of this choice\n",
    "or Keep them: if they have no bad leverages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms for each feature to visualize distribution\n",
    "df.hist(figsize=(12, 10), bins=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for each feature to detect outliers\n",
    "df.plot(kind='box', subplots=True, layout=(4,3), figsize=(12,12), sharex=False, sharey=False,)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Nb of outliers in each feature\n",
    "\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile) for the numerical columns\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1  # Interquartile range\n",
    "\n",
    "# Detect outliers in the dataset using the IQR method\n",
    "outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "# Count the number of outliers in each column\n",
    "outlier_count = outliers.sum()\n",
    "\n",
    "# Display the count of outliers for each column\n",
    "outlier_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think nothing is critical here, and the outliers, in my opinion, will not have heavy leverages as the total observations are reasonably high.\n",
    "The outliers represent real-world events that might indicate water contamination or poor performance of our AWG machine, they should be kept and analyzed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for each feature to detect outliers\n",
    "for column in df.columns[:-1]:  \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(data=df, x=\"Potability\", y=column, palette=[colors_pink[3], colors_blue[3]])\n",
    "    plt.title(f'Box Plot for {column}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Potability with individual features using box plots\n",
    "for column in df.columns[:-1]: \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.violinplot(x=\"Potability\", y=column, data=df, palette=[colors_pink[3], colors_blue[3]])\n",
    "    plt.title(f'Violin Plot for {column} vs Potability')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Skewness: \n",
    "It measures the asymmetry of the distribution of data around its mean.\n",
    "\n",
    "Skewness = 0: The data is perfectly symmetric (like a normal distribution).\n",
    "Positive Skewness: The tail on the right side of the distribution is longer or fatter than the left side (right-skewed).\n",
    "Negative Skewness: The tail on the left side of the distribution is longer or fatter than the right side (left-skewed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Calculate skewness for each numerical column\n",
    "skewness_values = df.skew()\n",
    "\n",
    "# Print skewness values\n",
    "print(\"Skewness values:\")\n",
    "print(skewness_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values close to 0 (like pH, Hardness, Chloramines, Organic Carbon, Turbidity) suggest that these attributes are approximately symmetric.\n",
    "Right skewed attributes:\n",
    "    Solids (0.62): Mildly right-skewed.\n",
    "    Potability (0.45): Slightly right-skewed.\n",
    "    Conductivity (0.26): Slightly right-skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Potability is a binary attribute so it can only have two values 0 and 1.\n",
    "- Solids and conductivity values are mainly concentrated respectively between (10 & 28K ppm) and (350 & 500 μS/cm).\n",
    "Since the skewness is mild, we will proceed without any transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Normality:  \n",
    "It refers to how closely the data follows a normal (Gaussian) distribution, which is a specific symmetric distribution where the mean, median, and mode are equal & the data follows the 68-95-99.7 rule (68% of data lies within 1 standard deviation of the mean, 95% within 2, and 99.7% within 3).\n",
    "\n",
    "\n",
    "- The Shapiro-Wilk Test checks if data follows a normal distribution. \n",
    "The null hypothesis H0 is that the data is normally distributed.\n",
    "    p-value > 0.05: Fail to reject H0 (data is normally distributed).\n",
    "    p-value < 0.05: Reject H0 (data is not normally distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Shapiro-Wilk test for normality for each numerical column\n",
    "shapiro_results = {}\n",
    "for column in df.select_dtypes(include=[float, int]).columns:\n",
    "    stat, p_value = shapiro(df[column].dropna())  # Drop missing values for the test\n",
    "    shapiro_results[column] = {\"Statistic\": stat, \"p-value\": p_value}\n",
    "\n",
    "# Convert Shapiro-Wilk results to a DataFrame and print them\n",
    "shapiro_results_df = pd.DataFrame.from_dict(shapiro_results, orient='index')\n",
    "print(\"\\nShapiro-Wilk Test Results:\")\n",
    "print(shapiro_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we notice non normal attributes, I won't be preforming any extensive normalization since it is not needed for our EDA, and the models we will proceed with are not sensitive to the scale or distribution of the data.\n",
    "\n",
    "\n",
    "We will simply proceed with standardazation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Standardization(Z-score normalization):\n",
    "+ Dividing train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Potability',axis=1).values\n",
    "y = df['Potability'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary purpose of this code is to prepare the data for a machine learning model by splitting it into training and testing sets and scaling the features. \n",
    "Scaling helps to improve the performance and convergence of many machine learning algorithms, especially those that rely on distance measurements, such as SVMs, K-nearest neighbors, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot checking: Spot checking is a great method to find out the baseline models for our data. It's quite easy and quick.\n",
    "The precision score is calculated to evaluate the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier, Perceptron\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define a list of models for spot-checking\n",
    "models = [\n",
    "    (\"LR\", LogisticRegression(max_iter=1000)),\n",
    "    (\"SVC\", SVC()),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=10)),\n",
    "    (\"DTC\", DecisionTreeClassifier()),\n",
    "    (\"GNB\", GaussianNB()),\n",
    "    (\"SGDC\", SGDClassifier()),\n",
    "    (\"Perc\", Perceptron()),\n",
    "    (\"NC\", NearestCentroid()),\n",
    "    (\"Ridge\", RidgeClassifier()),\n",
    "    (\"NuSVC\", NuSVC()),\n",
    "    (\"BNB\", BernoulliNB()),\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('ADA', AdaBoostClassifier()),\n",
    "    ('XGB', GradientBoostingClassifier()),\n",
    "    ('PAC', PassiveAggressiveClassifier()),\n",
    "    ('MLP', MLPClassifier(max_iter=1000)),  # Neural network (Multi-layer Perceptron)\n",
    "    ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')),  # XGBoost\n",
    "    ('LightGBM', LGBMClassifier()),  # LightGBM\n",
    "    ('CatBoost', CatBoostClassifier(verbose=0)),  # CatBoost (set verbose=0 to suppress output)\n",
    "    ('Neural Network', MLPClassifier(max_iter=1000))  # Neural network (Multi-layer Perceptron)\n",
    "]\n",
    "\n",
    "# Store results\n",
    "tresults = []\n",
    "names = []\n",
    "finalResults = []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    model_results = model.predict(X_test)\n",
    "    score = precision_score(y_test, model_results, average='macro')\n",
    "    tresults.append(score)\n",
    "    names.append(name)\n",
    "    finalResults.append((name, score))\n",
    "\n",
    "# Sort the results by precision score in descending order\n",
    "finalResults.sort(key=lambda k: k[1], reverse=True)\n",
    "\n",
    "# Display sorted results\n",
    "finalResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////////////////////////////////////////////\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe through spot checking out top rated models whom we will use. However I still wish to test the linear regression model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study each model individually. I will be testing the following models:\n",
    "Logistic Regression,\n",
    "Random Forest,\n",
    "Gradient Boosting XGB,\n",
    "SVM : Support Vector Machine Model,\n",
    "Catboost, \n",
    "LightGBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////////// Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection\n",
    "Recursive feature elimination\n",
    "\n",
    "Given an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts all samples as not potable (Potability = 0), which leads to poor performance for the potable class (Potability = 1).\n",
    "This imbalance suggests that the model is not learning to distinguish between the two classes well, likely due to data imbalance or insufficient feature signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address class imbalance in your logistic regression model, I will try techniques like Synthetic Minority Over-sampling Technique (SMOTE) or by adjusting the class weights directly.\n",
    "\n",
    "Method 1: Using SMOTE: \n",
    "SMOTE is an oversampling technique that generates synthetic samples for the minority class.\n",
    "\n",
    "Method 2: Using Class Weight: \n",
    "Adjusting class weights directly in the logistic regression model to penalize misclassifications of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "### Method 1: Using SMOTE to handle class imbalance ###\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize and train the Logistic Regression model on SMOTE data\n",
    "model_smote = LogisticRegression(random_state=42)\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "\n",
    "# Evaluate the SMOTE model\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_smote)\n",
    "conf_matrix_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "class_report_smote = classification_report(y_test, y_pred_smote)\n",
    "\n",
    "# Display the results for SMOTE\n",
    "print(\"=== SMOTE Logistic Regression Model ===\")\n",
    "print(f\"Accuracy: {accuracy_smote * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_smote)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_smote)\n",
    "\n",
    "\n",
    "### Method 2: Using Class Weight to handle class imbalance ###\n",
    "# Initialize and train the Logistic Regression model with balanced class weights\n",
    "model_weighted = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "model_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_weighted = model_weighted.predict(X_test)\n",
    "\n",
    "# Evaluate the weighted model\n",
    "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "conf_matrix_weighted = confusion_matrix(y_test, y_pred_weighted)\n",
    "class_report_weighted = classification_report(y_test, y_pred_weighted)\n",
    "\n",
    "# Display the results for Class Weight\n",
    "print(\"\\n=== Weighted Logistic Regression Model ===\")\n",
    "print(f\"Accuracy: {accuracy_weighted * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_weighted)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these techniques led to a drop in the accuracy. Therefore, they are obsolete in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try feature engineering.\n",
    "\n",
    "Interaction Features: Helps capture relationships between pairs of features that might be important but aren't captured by individual features.\n",
    "\n",
    "Polynomial Features: Captures non-linear relationships, making the model more expressive.\n",
    "\n",
    "Feature Selection: Ensures that only the most relevant features are used, which helps reduce overfitting and improves model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "#  1. Feature Engineering: Create Interaction Features\n",
    "# Create interaction terms between features\n",
    "poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "X_train_inter = poly.fit_transform(X_train)\n",
    "X_test_inter = poly.transform(X_test)\n",
    "\n",
    "# 2. Feature Engineering: Polynomial Features\n",
    "# Create polynomial features (degree 2)\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Combine interaction and polynomial features\n",
    "X_train_enhanced = np.hstack((X_train_inter, X_train_poly))\n",
    "X_test_enhanced = np.hstack((X_test_inter, X_test_poly))\n",
    "\n",
    "# 3. Feature Engineering: Feature Selection\n",
    "# Recursive Feature Elimination (RFE) to select important features\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "rfe = RFE(estimator=log_reg, n_features_to_select=6)  # You can adjust the number of features\n",
    "X_train_selected = rfe.fit_transform(X_train_enhanced, y_train)\n",
    "X_test_selected = rfe.transform(X_test_enhanced)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the logistic regression model on selected features\n",
    "model = LogisticRegression(random_state=42, max_iter=500)\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the enhanced model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(\"=== Enhanced Logistic Regression Model ===\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURVES\n",
    "\n",
    "\n",
    "# Compute probabilities for ROC curve and AUC\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test_selected)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#  Define the desired sensitivity (True Positive Rate)\n",
    "target_sensitivity = 0.90\n",
    "\n",
    "# Find the index where sensitivity (tpr) is greater than or equal to the target sensitivity\n",
    "index = np.where(tpr >= target_sensitivity)[0][0]\n",
    "\n",
    "# Get the corresponding threshold, sensitivity, specificity, and false positive rate\n",
    "threshold = thresholds[index]\n",
    "sensitivity = tpr[index]\n",
    "specificity = 1 - fpr[index]\n",
    "false_positive_rate = fpr[index] * 100\n",
    "\n",
    "# Display the information\n",
    "print(f\"Using a threshold of {threshold:.3f} guarantees a sensitivity of {sensitivity:.3f} \"\n",
    "      f\"and a specificity of {specificity:.3f}, i.e. a false positive rate of {false_positive_rate:.2f}%.\")\n",
    "\n",
    "#  Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Add the dashed line for random guessing\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)\n",
    "\n",
    "# Add vertical and horizontal lines at the desired sensitivity and corresponding threshold\n",
    "plt.axvline(x=fpr[index], color='blue', linestyle='--', lw=2)  # Vertical line (False Positive Rate)\n",
    "plt.axhline(y=tpr[index], color='blue', linestyle='--', lw=2)  # Horizontal line (True Positive Rate)\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Logistic Regression', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#  Compute Youden's Index (Sensitivity + Specificity - 1)\n",
    "youden_index = tpr - fpr\n",
    "optimal_index = np.argmax(youden_index)  # Index of the optimal threshold\n",
    "optimal_threshold = thresholds[optimal_index]\n",
    "\n",
    "# Get the corresponding sensitivity and specificity for the optimal threshold\n",
    "optimal_sensitivity = tpr[optimal_index]\n",
    "optimal_specificity = 1 - fpr[optimal_index]\n",
    "\n",
    "# Print the optimal threshold, sensitivity, and specificity\n",
    "print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"Sensitivity at optimal threshold: {optimal_sensitivity:.3f}\")\n",
    "print(f\"Specificity at optimal threshold: {optimal_specificity:.3f}\")\n",
    "\n",
    "#  Plot the ROC curve with the optimal threshold indicated\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "plt.axvline(x=fpr[optimal_index], color='blue', linestyle='--', lw=2)  # Vertical line at optimal FPR\n",
    "plt.axhline(y=tpr[optimal_index], color='blue', linestyle='--', lw=2)  # Horizontal line at optimal TPR\n",
    "plt.xlabel('False Positive Rate (1 - specificity)')\n",
    "plt.ylabel('True Positive Rate (recall)')\n",
    "plt.title('ROC Curve with Optimal Threshold for Logistic Regression')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the feature engineering led to an accuracy increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most important features for LR\n",
    "\n",
    "\n",
    "# Assuming `X_train` and `y_train` are your training data\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Convert X_train back to DataFrame to get column names\n",
    "X_train_df = pd.DataFrame(X_train, columns=df.drop('Potability', axis=1).columns)\n",
    "\n",
    "# Create a DataFrame with feature importance\n",
    "feature_importance = pd.Series(model.coef_[0], index=X_train_df.columns)\n",
    "feature_importance = feature_importance.abs().sort_values(ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance.plot(kind='bar', color= colors_pink[4])\n",
    "plt.title('Feature Importance After Engineering')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/////////////////////////////////\n",
    "\n",
    "RANDOM FOREST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Score:\n",
    "\n",
    "The accuracy score represents the proportion of correctly predicted instances compared to the total instances in the test set.\n",
    "For example, if the output was Accuracy: 0.75, it means that 75% of the water samples were correctly classified as potable or non-potable by the model.\n",
    "This metric gives a quick overall view of the model's performance, but it should be interpreted carefully, especially if the classes are imbalanced.\n",
    "Classification Report:\n",
    "\n",
    "The classification report provides more detailed metrics: Precision, Recall, F1-Score, and Support for each class (potable and non-potable).\n",
    "Precision (Positive Predictive Value): Measures the accuracy of positive predictions. A higher precision means fewer false positives.\n",
    "Example: A precision of 0.80 for potable water means that 80% of the samples predicted as potable are actually potable.\n",
    "Recall (Sensitivity or True Positive Rate): Measures the ability to identify all actual positive instances. A higher recall means fewer false negatives.\n",
    "Example: A recall of 0.70 for potable water indicates that the model correctly identifies 70% of the actual potable water samples.\n",
    "F1-Score: The harmonic mean of precision and recall. It balances the two metrics and is useful when dealing with class imbalances.\n",
    "Example: A high F1-score indicates a balance between precision and recall.\n",
    "Support: The number of true instances for each class in the test set.\n",
    "Confusion Matrix:\n",
    "\n",
    "The confusion matrix visually represents the model's performance by showing the counts of true positives, true negatives, false positives, and false negatives.\n",
    "Interpretation of the Matrix:\n",
    "True Positives (TP): Correctly predicted potable samples.\n",
    "True Negatives (TN): Correctly predicted non-potable samples.\n",
    "False Positives (FP): Samples predicted as potable but are actually non-potable (Type I error).\n",
    "False Negatives (FN): Samples predicted as non-potable but are actually potable (Type II error).\n",
    "A high number of TP and TN with low FP and FN indicates good model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the test set\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_rf)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Define a desired sensitivity (for example, 0.90)\n",
    "target_sensitivity = 0.90\n",
    "\n",
    "# Find the threshold corresponding to the desired sensitivity\n",
    "index = np.where(tpr >= target_sensitivity)[0][0]\n",
    "threshold = thresholds[index]\n",
    "\n",
    "# Get the corresponding sensitivity, specificity, and false positive rate\n",
    "sensitivity = tpr[index]\n",
    "specificity = 1 - fpr[index]\n",
    "false_positive_rate = fpr[index] * 100\n",
    "\n",
    "# Print the selected threshold, sensitivity, and specificity\n",
    "print(f\"Using a threshold of {threshold:.3f} guarantees a sensitivity of {sensitivity:.3f} \"\n",
    "      f\"and a specificity of {specificity:.3f}, i.e. a false positive rate of {false_positive_rate:.2f}%.\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "\n",
    "# Add vertical and horizontal lines for the selected threshold\n",
    "plt.axvline(x=fpr[index], color='blue', linestyle='--', lw=2)  # Vertical line at selected FPR\n",
    "plt.axhline(y=tpr[index], color='blue', linestyle='--', lw=2)  # Horizontal line at selected TPR\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=12)\n",
    "plt.title('Receiver operating characteristic (ROC) curve with selected threshold (Random Forest)', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute Youden's Index (Sensitivity + Specificity - 1)\n",
    "youden_index = tpr - fpr\n",
    "optimal_index = np.argmax(youden_index)  # Index of the optimal threshold\n",
    "optimal_threshold = thresholds[optimal_index]\n",
    "\n",
    "# Get the corresponding sensitivity and specificity for the optimal threshold\n",
    "optimal_sensitivity = tpr[optimal_index]\n",
    "optimal_specificity = 1 - fpr[optimal_index]\n",
    "\n",
    "# Print the optimal threshold, sensitivity, and specificity\n",
    "print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"Sensitivity at optimal threshold: {optimal_sensitivity:.3f}\")\n",
    "print(f\"Specificity at optimal threshold: {optimal_specificity:.3f}\")\n",
    "\n",
    "# Plot the ROC curve with the optimal threshold indicated\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "plt.axvline(x=fpr[optimal_index], color='blue', linestyle='--', lw=2)  # Vertical line at optimal FPR\n",
    "plt.axhline(y=tpr[optimal_index], color='blue', linestyle='--', lw=2)  # Horizontal line at optimal TPR\n",
    "plt.xlabel('False Positive Rate (1 - specificity)')\n",
    "plt.ylabel('True Positive Rate (recall)')\n",
    "plt.title('Receiver operating characteristic (ROC) curve with optimal threshold (Random Forest)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better improve the model, I will try model tuning using RandomizedSearchCV, while addressing class imbalance with Synthetic Minority Over-sampling Technique (SMOTE) to help balance the classes by generating synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Random Forest model with parameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Randomized Search with Cross-Validation\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from RandomizedSearchCV\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\\n\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the test set\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_rf)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Define a desired sensitivity (for example, 0.90)\n",
    "target_sensitivity = 0.90\n",
    "\n",
    "# Find the threshold corresponding to the desired sensitivity\n",
    "index = np.where(tpr >= target_sensitivity)[0][0]\n",
    "threshold = thresholds[index]\n",
    "\n",
    "# Get the corresponding sensitivity, specificity, and false positive rate\n",
    "sensitivity = tpr[index]\n",
    "specificity = 1 - fpr[index]\n",
    "false_positive_rate = fpr[index] * 100\n",
    "\n",
    "# Print the selected threshold, sensitivity, and specificity\n",
    "print(f\"Using a threshold of {threshold:.3f} guarantees a sensitivity of {sensitivity:.3f} \"\n",
    "      f\"and a specificity of {specificity:.3f}, i.e. a false positive rate of {false_positive_rate:.2f}%.\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "\n",
    "# Add vertical and horizontal lines for the selected threshold\n",
    "plt.axvline(x=fpr[index], color='blue', linestyle='--', lw=2)  # Vertical line at selected FPR\n",
    "plt.axhline(y=tpr[index], color='blue', linestyle='--', lw=2)  # Horizontal line at selected TPR\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=12)\n",
    "plt.title('Receiver operating characteristic (ROC) curve with selected threshold (Random Forest)', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute Youden's Index (Sensitivity + Specificity - 1)\n",
    "youden_index = tpr - fpr\n",
    "optimal_index = np.argmax(youden_index)  # Index of the optimal threshold\n",
    "optimal_threshold = thresholds[optimal_index]\n",
    "\n",
    "# Get the corresponding sensitivity and specificity for the optimal threshold\n",
    "optimal_sensitivity = tpr[optimal_index]\n",
    "optimal_specificity = 1 - fpr[optimal_index]\n",
    "\n",
    "# Print the optimal threshold, sensitivity, and specificity\n",
    "print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"Sensitivity at optimal threshold: {optimal_sensitivity:.3f}\")\n",
    "print(f\"Specificity at optimal threshold: {optimal_specificity:.3f}\")\n",
    "\n",
    "# Plot the ROC curve with the optimal threshold indicated\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "plt.axvline(x=fpr[optimal_index], color='blue', linestyle='--', lw=2)  # Vertical line at optimal FPR\n",
    "plt.axhline(y=tpr[optimal_index], color='blue', linestyle='--', lw=2)  # Horizontal line at optimal TPR\n",
    "plt.xlabel('False Positive Rate (1 - specificity)')\n",
    "plt.ylabel('True Positive Rate (recall)')\n",
    "plt.title('Receiver operating characteristic (ROC) curve with optimal threshold (Random Forest)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy slightly increased.////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////\n",
    "SVC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to optimize svc params\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create a pipeline with StandardScaler and SVC\n",
    "svc_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale the features\n",
    "    ('svc', SVC(kernel='linear'))  # Step 2: Train the SVC model\n",
    "])\n",
    "\n",
    "# Train the SVC model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svc_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Model Evaluation\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\\n\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the pipeline steps\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters to search for the SVC model\n",
    "param_grid = {\n",
    "    'svc__kernel': ['linear', 'rbf'],  # Try both linear and RBF kernel\n",
    "    'svc__C': [0.1, 1, 10]  # Try different values of regularization\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL VERSION?\n",
    "This code explores a wide range of hyperparameters with RandomizedSearchCV and then fine-tunes the SVC model with GridSearchCV. The final SVC model is returned with the optimal hyperparameters. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Define the pipeline with scaling and SVC model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scaling the features\n",
    "    ('svc', SVC(random_state=42))  # Support Vector Classifier\n",
    "])\n",
    "\n",
    "# 2- RandomizedSearchCV for broad hyperparameter exploration\n",
    "param_dist = {\n",
    "    'svc__kernel': ['linear', 'rbf'],  # Try both linear and RBF kernels\n",
    "    'svc__C': stats.uniform(0.1, 100),  # Uniform distribution for C (regularization parameter)\n",
    "    'svc__gamma': ['scale', 'auto', 0.1, 1, 10]  # Gamma for RBF kernel\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV to explore the hyperparameter space\n",
    "random_search = RandomizedSearchCV(pipeline, param_dist, n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and cross-validation accuracy from RandomizedSearchCV\n",
    "print(\"Best Parameters from Randomized Search:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy from Randomized Search:\", random_search.best_score_)\n",
    "\n",
    "# 3- Use the best hyperparameters to narrow down the grid for fine-tuning with GridSearchCV\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Narrow down the search space based on RandomizedSearchCV results\n",
    "param_grid = {\n",
    "    'svc__kernel': [best_params['svc__kernel']],  # Use the best kernel\n",
    "    'svc__C': [best_params['svc__C'] * 0.5, best_params['svc__C'], best_params['svc__C'] * 2],  # Fine-tuning C\n",
    "    'svc__gamma': [best_params['svc__gamma']] if best_params['svc__kernel'] == 'linear' else [best_params['svc__gamma'] * 0.5, best_params['svc__gamma'], best_params['svc__gamma'] * 2]  # Fine-tuning gamma for RBF\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to fine-tune the model\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters from GridSearchCV\n",
    "print(\"Best Parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy from Grid Search:\", grid_search.best_score_)\n",
    "\n",
    "# 4- Use the best model from GridSearchCV to make predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Return the best model (SVC with the best hyperparameters)\n",
    "svc_model = best_model\n",
    "svc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "# Predict decision function scores for the test set (used for SVC instead of predict_proba)\n",
    "y_pred_decision = svc_model.decision_function(X_test)  # Get decision function scores\n",
    "\n",
    "# Compute ROC curve and AUC using decision function scores\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_decision)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Define the desired sensitivity (True Positive Rate)\n",
    "target_sensitivity = 0.90\n",
    "\n",
    "# Find the index where sensitivity (tpr) is greater than or equal to the target sensitivity\n",
    "index = np.where(tpr >= target_sensitivity)[0][0]\n",
    "\n",
    "# Get the corresponding threshold, sensitivity, specificity, and false positive rate\n",
    "threshold = thresholds[index]\n",
    "sensitivity = tpr[index]\n",
    "specificity = 1 - fpr[index]\n",
    "false_positive_rate = fpr[index] * 100\n",
    "\n",
    "# Display the information\n",
    "print(f\"Using a threshold of {threshold:.3f} guarantees a sensitivity of {sensitivity:.3f} \"\n",
    "      f\"and a specificity of {specificity:.3f}, i.e. a false positive rate of {false_positive_rate:.2f}%.\") \n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "\n",
    "# Add the dashed line for random guessing\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)\n",
    "\n",
    "# Add vertical and horizontal lines at the desired sensitivity and corresponding threshold\n",
    "plt.axvline(x=fpr[index], color='blue', linestyle='--', lw=2)  # Vertical line (False Positive Rate)\n",
    "plt.axhline(y=tpr[index], color='blue', linestyle='--', lw=2)  # Horizontal line (True Positive Rate)\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=12)\n",
    "plt.title('Receiver operating characteristic (ROC) curve', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute Youden's Index (Sensitivity + Specificity - 1)\n",
    "youden_index = tpr - fpr\n",
    "optimal_index = np.argmax(youden_index)  # Index of the optimal threshold\n",
    "optimal_threshold = thresholds[optimal_index]\n",
    "\n",
    "# Get the corresponding sensitivity and specificity for the optimal threshold\n",
    "optimal_sensitivity = tpr[optimal_index]\n",
    "optimal_specificity = 1 - fpr[optimal_index]\n",
    "\n",
    "# Print the optimal threshold, sensitivity, and specificity\n",
    "print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"Sensitivity at optimal threshold: {optimal_sensitivity:.3f}\")\n",
    "print(f\"Specificity at optimal threshold: {optimal_specificity:.3f}\")\n",
    "\n",
    "# Plot the ROC curve with the optimal threshold indicated\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "plt.axvline(x=fpr[optimal_index], color='blue', linestyle='--', lw=2)  # Vertical line at optimal FPR\n",
    "plt.axhline(y=tpr[optimal_index], color='blue', linestyle='--', lw=2)  # Horizontal line at optimal TPR\n",
    "plt.xlabel('False Positive Rate (1 - specificity)')\n",
    "plt.ylabel('True Positive Rate (recall)')\n",
    "plt.title('Receiver operating characteristic (ROC) curve with optimal threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//////////////////////////////////// XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_xgboost = xgb_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgboost)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Define the desired sensitivity (True Positive Rate)\n",
    "target_sensitivity = 0.90\n",
    "\n",
    "# Find the index where sensitivity (tpr) is greater than or equal to the target sensitivity\n",
    "index = np.where(tpr >= target_sensitivity)[0][0]\n",
    "\n",
    "# Get the corresponding threshold, sensitivity, specificity, and false positive rate\n",
    "threshold = thresholds[index]\n",
    "sensitivity = tpr[index]\n",
    "specificity = 1 - fpr[index]\n",
    "false_positive_rate = fpr[index] * 100\n",
    "\n",
    "# Display the information\n",
    "print(f\"Using a threshold of {threshold:.3f} guarantees a sensitivity of {sensitivity:.3f} \"\n",
    "      f\"and a specificity of {specificity:.3f}, i.e. a false positive rate of {false_positive_rate:.2f}%.\")\n",
    "\n",
    "\n",
    "# Plot the ROC curve\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "\n",
    "# Add the dashed line for random guessing\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)\n",
    "\n",
    "# Add vertical and horizontal lines at the desired sensitivity and corresponding threshold\n",
    "plt.axvline(x=fpr[index], color='blue', linestyle='--', lw=2)  # Vertical line (False Positive Rate)\n",
    "plt.axhline(y=tpr[index], color='blue', linestyle='--', lw=2)  # Horizontal line (True Positive Rate)\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=12)\n",
    "plt.title('Receiver operating characteristic (ROC) curve', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute Youden's Index (Sensitivity + Specificity - 1)\n",
    "youden_index = tpr - fpr\n",
    "optimal_index = np.argmax(youden_index)  # Index of the optimal threshold\n",
    "optimal_threshold = thresholds[optimal_index]\n",
    "\n",
    "# Get the corresponding sensitivity and specificity for the optimal threshold\n",
    "optimal_sensitivity = tpr[optimal_index]\n",
    "optimal_specificity = 1 - fpr[optimal_index]\n",
    "\n",
    "# Print the optimal threshold, sensitivity, and specificity\n",
    "print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"Sensitivity at optimal threshold: {optimal_sensitivity:.3f}\")\n",
    "print(f\"Specificity at optimal threshold: {optimal_specificity:.3f}\")\n",
    "\n",
    "# Plot the ROC curve with the optimal threshold indicated\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "plt.axvline(x=fpr[optimal_index], color='blue', linestyle='--', lw=2)  # Vertical line at optimal FPR\n",
    "plt.axhline(y=tpr[optimal_index], color='blue', linestyle='--', lw=2)  # Horizontal line at optimal TPR\n",
    "plt.xlabel('False Positive Rate (1 - specificity)')\n",
    "plt.ylabel('True Positive Rate (recall)')\n",
    "plt.title('Receiver operating characteristic (ROC) curve with optimal threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youden’s Index balances both sensitivity and specificity. It finds a threshold where the model is good at detecting both diseased and healthy people. This is a more general approach.\n",
    "\n",
    "Formula:\n",
    "J=Sensitivity+Specificity−1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Dictionary to store accuracies\n",
    "accuracy_results = {}\n",
    "\n",
    "# 1. Baseline XGBoost Model\n",
    "xgb_model_baseline = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model_baseline.fit(X_train, y_train)\n",
    "y_pred_baseline = xgb_model_baseline.predict(X_test)\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "accuracy_results['Baseline XGBoost'] = accuracy_baseline\n",
    "\n",
    "# 2. XGBoost with Missing Data Handling (Imputation)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "xgb_model_imputed = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model_imputed.fit(X_train_imputed, y_train)\n",
    "y_pred_imputed = xgb_model_imputed.predict(X_test_imputed)\n",
    "accuracy_imputed = accuracy_score(y_test, y_pred_imputed)\n",
    "accuracy_results['XGBoost with Imputation'] = accuracy_imputed\n",
    "\n",
    "# 3. XGBoost with SMOTE for Imbalanced Classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "xgb_model_smote = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = xgb_model_smote.predict(X_test)\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_smote)\n",
    "accuracy_results['XGBoost with SMOTE'] = accuracy_smote\n",
    "\n",
    "# 4. XGBoost with Hyperparameter Tuning\n",
    "xgb_model_tuned = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', max_depth=5, n_estimators=300, learning_rate=0.1, random_state=42)\n",
    "xgb_model_tuned.fit(X_train, y_train)\n",
    "y_pred_tuned = xgb_model_tuned.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "accuracy_results['XGBoost with Tuning'] = accuracy_tuned\n",
    "\n",
    "# Print all accuracies for comparison\n",
    "for model_version, accuracy in accuracy_results.items():\n",
    "    print(f\"{model_version}: Accuracy = {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/////////////////Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Hyperparameter Optimization using Optuna, which is a library that uses Bayesian optimization to find the best hyperparameters more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import catboost as cb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.2),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_strength': trial.suggest_int('random_strength', 1, 20),\n",
    "        'random_state': 42,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    # Train CatBoost\n",
    "    model = cb.CatBoostClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Return the accuracy score for optimization\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Initialize Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(f\"Best Parameters: {study.best_params}\")\n",
    "print(f\"Best Accuracy: {study.best_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import catboost as cb\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter distribution for random search\n",
    "param_dist = {\n",
    "    'depth': np.arange(4, 11, 1),\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),\n",
    "    'iterations': np.arange(100, 1001, 100),\n",
    "    'l2_leaf_reg': np.arange(1, 10, 2)\n",
    "}\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "catboost_model = cb.CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=catboost_model, param_distributions=param_dist, \n",
    "                                   n_iter=50, cv=5, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "\n",
    "# Get the best estimator (model)\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import catboost as cb\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'iterations': [100, 200, 300],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "}\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "catboost_model = cb.CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Get the best estimator (model)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding our best parameters, we build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, auc, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the CatBoost classifier\n",
    "catboost_model = cb.CatBoostClassifier(verbose=0, random_state=42,)\n",
    "\n",
    "# Train the model\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Compute ROC curve\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_catboost = catboost_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_catboost)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Define the desired sensitivity (True Positive Rate)\n",
    "target_sensitivity = 0.90\n",
    "\n",
    "# Find the index where sensitivity (tpr) is greater than or equal to the target sensitivity\n",
    "index = np.where(tpr >= target_sensitivity)[0][0]\n",
    "\n",
    "# Get the corresponding threshold, sensitivity, specificity, and false positive rate\n",
    "threshold = thresholds[index]\n",
    "sensitivity = tpr[index]\n",
    "specificity = 1 - fpr[index]\n",
    "false_positive_rate = fpr[index] * 100\n",
    "\n",
    "# Display the information\n",
    "print(f\"Using a threshold of {threshold:.3f} guarantees a sensitivity of {sensitivity:.3f} \"\n",
    "      f\"and a specificity of {specificity:.3f}, i.e. a false positive rate of {false_positive_rate:.2f}%.\")\n",
    "\n",
    "\n",
    "# Plot the ROC curve\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "\n",
    "# Add the dashed line for random guessing\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)\n",
    "\n",
    "# Add vertical and horizontal lines at the desired sensitivity and corresponding threshold\n",
    "plt.axvline(x=fpr[index], color='blue', linestyle='--', lw=2)  # Vertical line (False Positive Rate)\n",
    "plt.axhline(y=tpr[index], color='blue', linestyle='--', lw=2)  # Horizontal line (True Positive Rate)\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=12)\n",
    "plt.title('Receiver operating characteristic (ROC) curve', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute Youden's Index (Sensitivity + Specificity - 1)\n",
    "youden_index = tpr - fpr\n",
    "optimal_index = np.argmax(youden_index)  # Index of the optimal threshold\n",
    "optimal_threshold = thresholds[optimal_index]\n",
    "\n",
    "# Get the corresponding sensitivity and specificity for the optimal threshold\n",
    "optimal_sensitivity = tpr[optimal_index]\n",
    "optimal_specificity = 1 - fpr[optimal_index]\n",
    "\n",
    "# Print the optimal threshold, sensitivity, and specificity\n",
    "print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"Sensitivity at optimal threshold: {optimal_sensitivity:.3f}\")\n",
    "print(f\"Specificity at optimal threshold: {optimal_specificity:.3f}\")\n",
    "\n",
    "# Plot the ROC curve with the optimal threshold indicated\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "plt.axvline(x=fpr[optimal_index], color='blue', linestyle='--', lw=2)  # Vertical line at optimal FPR\n",
    "plt.axhline(y=tpr[optimal_index], color='blue', linestyle='--', lw=2)  # Horizontal line at optimal TPR\n",
    "plt.xlabel('False Positive Rate (1 - specificity)')\n",
    "plt.ylabel('True Positive Rate (recall)')\n",
    "plt.title('Receiver operating characteristic (ROC) curve with optimal threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////////LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    # Initialize and train LightGBM model\n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and return accuracy score\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(f\"Best Parameters: {study.best_params}\")\n",
    "print(f\"Best Accuracy: {study.best_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "lightgbm_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lightgbm_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters from GridSearchCV: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning with Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'num_leaves': np.arange(20, 150, 10),\n",
    "    'max_depth': np.arange(3, 16, 2),\n",
    "    'learning_rate': np.logspace(-3, 0, 10),  # From 0.001 to 1\n",
    "    'n_estimators': np.arange(100, 1001, 100),\n",
    "    'min_child_samples': np.arange(10, 100, 10),\n",
    "    'subsample': np.linspace(0.6, 1.0, 5),\n",
    "    'colsample_bytree': np.linspace(0.6, 1.0, 5),\n",
    "    'reg_alpha': np.logspace(-4, 0, 5),\n",
    "    'reg_lambda': np.logspace(-4, 0, 5)\n",
    "}\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "lightgbm_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lightgbm_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    scoring='accuracy',  # You can also use 'roc_auc', 'f1', etc.\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Train the model using the best parameters\n",
    "lightgbm_model_best = lgb.LGBMClassifier(**best_params, random_state=42)\n",
    "lightgbm_model_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lightgbm_model_best.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, auc, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the LightGBM classifier\n",
    "lightgbm_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lightgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lightgbm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Compute ROC curve\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_lightgbm = lightgbm_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_lightgbm)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Define the desired sensitivity (True Positive Rate)\n",
    "target_sensitivity = 0.90\n",
    "\n",
    "# Find the index where sensitivity (tpr) is greater than or equal to the target sensitivity\n",
    "index = np.where(tpr >= target_sensitivity)[0][0]\n",
    "\n",
    "# Get the corresponding threshold, sensitivity, specificity, and false positive rate\n",
    "threshold = thresholds[index]\n",
    "sensitivity = tpr[index]\n",
    "specificity = 1 - fpr[index]\n",
    "false_positive_rate = fpr[index] * 100\n",
    "\n",
    "# Display the information\n",
    "print(f\"Using a threshold of {threshold:.3f} guarantees a sensitivity of {sensitivity:.3f} \"\n",
    "      f\"and a specificity of {specificity:.3f}, i.e. a false positive rate of {false_positive_rate:.2f}%.\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "\n",
    "# Add the dashed line for random guessing\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)\n",
    "\n",
    "# Add vertical and horizontal lines at the desired sensitivity and corresponding threshold\n",
    "plt.axvline(x=fpr[index], color='blue', linestyle='--', lw=2)  # Vertical line (False Positive Rate)\n",
    "plt.axhline(y=tpr[index], color='blue', linestyle='--', lw=2)  # Horizontal line (True Positive Rate)\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=12)\n",
    "plt.title('Receiver operating characteristic (ROC) curve', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute Youden's Index (Sensitivity + Specificity - 1)\n",
    "youden_index = tpr - fpr\n",
    "optimal_index = np.argmax(youden_index)  # Index of the optimal threshold\n",
    "optimal_threshold = thresholds[optimal_index]\n",
    "\n",
    "# Get the corresponding sensitivity and specificity for the optimal threshold\n",
    "optimal_sensitivity = tpr[optimal_index]\n",
    "optimal_specificity = 1 - fpr[optimal_index]\n",
    "\n",
    "# Print the optimal threshold, sensitivity, and specificity\n",
    "print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"Sensitivity at optimal threshold: {optimal_sensitivity:.3f}\")\n",
    "print(f\"Specificity at optimal threshold: {optimal_specificity:.3f}\")\n",
    "\n",
    "# Plot the ROC curve with the optimal threshold indicated\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=2)  # Diagonal line for random guessing\n",
    "plt.axvline(x=fpr[optimal_index], color='blue', linestyle='--', lw=2)  # Vertical line at optimal FPR\n",
    "plt.axhline(y=tpr[optimal_index], color='blue', linestyle='--', lw=2)  # Horizontal line at optimal TPR\n",
    "plt.xlabel('False Positive Rate (1 - specificity)')\n",
    "plt.ylabel('True Positive Rate (recall)')\n",
    "plt.title('Receiver operating characteristic (ROC) curve with optimal threshold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "////////////////////////////////Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"SVC\": SVC(probability=True, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Cat Boost\": cb.CatBoostClassifier(verbose=0, random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"Confusion Matrix\": cm\n",
    "    }\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    results[name] = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Results with no normalization -->\n",
    "\n",
    "Model: Random Forest\n",
    "Accuracy: 0.6785350966429298\n",
    "Precision: 0.6584158415841584\n",
    "Recall: 0.35\n",
    "F1 Score: 0.4570446735395189\n",
    "ROC AUC: 0.6757091734310902\n",
    "Confusion Matrix: [[534  69]\n",
    " [247 133]]\n",
    "------------------------------\n",
    "Model: XGBoost\n",
    "Accuracy: 0.6541200406917599\n",
    "Precision: 0.5684931506849316\n",
    "Recall: 0.4368421052631579\n",
    "F1 Score: 0.49404761904761907\n",
    "ROC AUC: 0.6609321811992668\n",
    "Confusion Matrix: [[477 126]\n",
    " [214 166]]\n",
    "------------------------------\n",
    "Model: SVC\n",
    "Accuracy: 0.6785350966429298\n",
    "Precision: 0.7222222222222222\n",
    "Recall: 0.2736842105263158\n",
    "F1 Score: 0.3969465648854962\n",
    "ROC AUC: 0.6996268656716419\n",
    "Confusion Matrix: [[563  40]\n",
    " [276 104]]\n",
    "------------------------------\n",
    "Model: Logistic Regression\n",
    "Accuracy: 0.6134282807731435\n",
    "Precision: 0.0\n",
    "Recall: 0.0\n",
    "F1 Score: 0.0\n",
    "ROC AUC: 0.5131448023042682\n",
    "Confusion Matrix: [[603   0]\n",
    " [380   0]]\n",
    "------------------------------\n",
    "Model: Cat Boost\n",
    "Accuracy: 0.6754832146490336\n",
    "Precision: 0.6380090497737556\n",
    "Recall: 0.37105263157894736\n",
    "F1 Score: 0.46921797004991683\n",
    "ROC AUC: 0.6931002880335166\n",
    "Confusion Matrix: [[523  80]\n",
    " [239 141]]\n",
    "------------------------------\n",
    "Model: LightGBM\n",
    "Accuracy: 0.6673448626653102\n",
    "Precision: 0.6\n",
    "Recall: 0.41842105263157897\n",
    "F1 Score: 0.4930232558139535\n",
    "ROC AUC: 0.6622632451776207\n",
    "Confusion Matrix: [[497 106]\n",
    " [221 159]]\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for visualization\n",
    "metrics_list = []\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    metrics_list.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": metrics[\"Accuracy\"],\n",
    "        \"Precision\": metrics[\"Precision\"],\n",
    "        \"Recall\": metrics[\"Recall\"],\n",
    "        \"F1 Score\": metrics[\"F1 Score\"],\n",
    "        \"ROC AUC\": metrics[\"ROC AUC\"]\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Plotting the metrics using Plotly for interactivity\n",
    "fig = px.bar(\n",
    "    metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Score'),\n",
    "    x='Model', y='Score', color='Metric', barmode='group',\n",
    "    hover_data={'Score': ':.2f'},  \n",
    "    title='Model Comparison on Various Metrics',\n",
    "    color_discrete_sequence=colors_pink \n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    legend_title='Metric',\n",
    "    title_x=0.5,\n",
    "    width=1500,  \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Plotting confusion matrices\n",
    "for model_name, cm in results.items():\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm[\"Confusion Matrix\"], annot=True, fmt='d', cmap= \"Blues\")  # Custom pinkish cmap\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc, roc_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc_value = auc(fpr, tpr)\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"Confusion Matrix\": cm,\n",
    "        \"Precision-Recall AUC\": pr_auc,\n",
    "        \"Precision Curve\": precision_curve,\n",
    "        \"Recall Curve\": recall_curve,\n",
    "        \"ROC AUC Value\": roc_auc_value,\n",
    "        \"FPR\": fpr,\n",
    "        \"TPR\": tpr\n",
    "    }\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    results[name] = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in [\"Precision Curve\", \"Recall Curve\", \"FPR\", \"TPR\"]:\n",
    "            continue\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "plt.figure(figsize=(10, 7))\n",
    "for model_name, metrics in results.items():\n",
    "    plt.plot(metrics[\"Recall Curve\"], metrics[\"Precision Curve\"], label=f\"{model_name} (PR AUC = {metrics['Precision-Recall AUC']:.2f})\")\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 7))\n",
    "for model_name, metrics in results.items():\n",
    "    plt.plot(metrics[\"FPR\"], metrics[\"TPR\"], label=f\"{model_name} (ROC AUC = {metrics['ROC AUC Value']:.2f})\")\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take into consideration: \n",
    "\n",
    "\n",
    "Imbalanced Classes: Check if the classes (potable vs. non-potable water) are balanced. Imbalanced classes can make metrics like accuracy misleading because the model might simply predict the majority class.\n",
    "\n",
    "Importance of False Positives vs. False Negatives:\n",
    "\n",
    "False Positives: Predicting water is potable when it is not (safety risk).\n",
    "False Negatives: Predicting water is non-potable when it is safe (unnecessary waste).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be Used Approach:\n",
    "Primary Metric: Use F1 Score if you want a balanced performance between detecting true positives and avoiding false positives.\n",
    "Secondary Metric: Consider ROC AUC to understand how well the model distinguishes between potable and non-potable water across different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score: Combines precision and recall into a single score, balancing both false positives and false negatives. This is especially useful if the dataset is imbalanced or when you need to balance the importance of detecting potable water correctly and avoiding false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC (Area Under the Receiver Operating Characteristic Curve):\n",
    "\n",
    "Measures the trade-off between the true positive rate and false positive rate.\n",
    "Useful when you care about the model's ability to distinguish between classes at various thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall AUC:\n",
    "\n",
    "Especially valuable in imbalanced datasets where the minority class (potable water) is of particular interest.\n",
    "Focuses on the model's performance on positive cases without being influenced by true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Btw Recall is sensitivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> We can notice that LightGBM has the highest ROC AUC value, PR AUC, and Accuracy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
